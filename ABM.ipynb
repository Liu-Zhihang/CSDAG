{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic data handling\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Data visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "import seaborn as sns\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "\n",
    "# Utilities\n",
    "import json\n",
    "import re\n",
    "import math\n",
    "from collections import Counter\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "# Parallel processing and serialization\n",
    "import tqdm\n",
    "import joblib\n",
    "\n",
    "# Statistical modeling\n",
    "from scipy.stats.qmc import LatinHypercube\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('官方媒体/官媒清单.txt', 'r') as f:\n",
    "    data = f.read()\n",
    "    data = data.split('\\n')\n",
    "m_media = [item for item in data if item != '']\n",
    "data = pd.read_excel('./output/data.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mid_name = {}\n",
    "for mid, name in data[['mid', 'user_name']].values:\n",
    "    mid_name[mid] = name\n",
    "mid_verify_typ = {}\n",
    "for mid, verified in data[['mid', 'verify_typ']].values:\n",
    "    mid_verify_typ[mid] = verified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forwarding = pd.read_csv('./data/新冠topic_reposts_用户数据/新冠topic_reposts.csv')\n",
    "forwarding['origin_name'] = forwarding['origin_mid'].apply(lambda x: mid_name.get(x, ''))\n",
    "forwarding['origin_verify_typ'] = forwarding['origin_mid'].apply(lambda x: mid_verify_typ.get(x, ''))\n",
    "forwarding = forwarding[forwarding['认证类型'] != '暂不爬取']\n",
    "# 去除认证类型为缺失值的行\n",
    "forwarding = forwarding.dropna(subset=['认证类型'])\n",
    "forwarding['认证类型'] = forwarding['认证类型'].astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1和3还有在官媒清单里代表官媒，[0, 200, 220]代表自媒体，-1代表普通用户\n",
    "forwarding['d_type'] = 'None'\n",
    "forwarding.loc[forwarding['认证类型'] == 1, 'd_type'] = 'm_media'\n",
    "forwarding.loc[forwarding['认证类型'] == 3, 'd_type'] = 'w_media'\n",
    "forwarding.loc[forwarding['认证类型'] == 0, 'd_type'] = 'w_media'\n",
    "forwarding.loc[forwarding['认证类型'] == 200, 'd_type'] = 'w_media'\n",
    "forwarding.loc[forwarding['认证类型'] == 220, 'd_type'] = 'w_media'\n",
    "forwarding.loc[forwarding['认证类型'] == -1, 'd_type'] = 'o_people'\n",
    "forwarding.loc[forwarding['user_name'].isin(m_media), 'd_type'] = 'm_media'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_media_name = forwarding[forwarding['d_type'] == 'm_media']['user_name']\n",
    "w_media_name = forwarding[forwarding['d_type'] == 'w_media']['user_name']\n",
    "m_media_name = m_media_name.drop_duplicates() \n",
    "m_media_name = m_media_name.to_list()\n",
    "m_media_name = m_media_name + m_media\n",
    "m_media_name = list(set(m_media_name))\n",
    "\n",
    "w_media_name = w_media_name.drop_duplicates()\n",
    "w_media_name = w_media_name.to_list()\n",
    "w_media_name = list(set(w_media_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1234)\n",
    "forwarding['o_type'] = 'None'\n",
    "forwarding.loc[forwarding['origin_verify_typ'] == '没有认证', 'o_type'] = 'o_people'\n",
    "# forwarding.loc[forwarding['origin_verify_typ'] != '没有认证', 'o_type'] = 'w_media'\n",
    "forwarding.loc[forwarding['origin_name'].isin(w_media_name), 'o_type'] = 'w_media'\n",
    "forwarding.loc[forwarding['origin_name'].isin(m_media_name), 'o_type'] = 'm_media'\n",
    "# 统计每天的转发量，利用publish_time获取日期\n",
    "forwarding['publish_time'] = pd.to_datetime(forwarding['publish_time'])\n",
    "forwarding['date'] = forwarding['publish_time'].dt.date\n",
    "forwarding['date'] = pd.to_datetime(forwarding['date'])\n",
    "forwarding_sample = forwarding[('2022-10-01' <= forwarding['date'])&(forwarding['date'] <= '2023-01-18')]\n",
    "# forwarding_sample.groupby('date')['is_mainstream'].sum().sort_values(ascending=False).reset_index().to_csv('官方媒体/官媒转发量.csv', index=False)\n",
    "forwarding_sample = forwarding_sample.sample(frac=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forwarding_sample.loc[forwarding_sample['o_type'] == 'None', 'o_type'] = 'w_media'\n",
    "links = forwarding_sample[['origin_name', 'o_type', 'user_name', 'd_type']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "links = links[links['d_type'] != 'None']\n",
    "links = links.drop_duplicates()\n",
    "links = links[~ ((links['o_type'] == 'o_people') & (links['d_type'] == 'o_people'))]\n",
    "links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_sentiment = pd.read_csv('./output/sentiment/data_sentiment.csv')\n",
    "data_risk_mainstream = pd.read_csv('./output/risk_media/data_risk_mainstream.csv')\n",
    "data_risk_wemedia = pd.read_csv('./output/risk_media/data_risk_wemedia.csv')\n",
    "data_sentiment.dropna(subset=['sentiment'], inplace=True)\n",
    "data_risk_mainstream.dropna(subset=['risk'], inplace=True)\n",
    "data_risk_wemedia.dropna(subset=['risk'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 如果用户重复出现，那么取最近值\n",
    "user_sentiment = {}\n",
    "for name, sentiment in data_sentiment[['user_name', 'sentiment']].values:\n",
    "    if sentiment == 'high':\n",
    "        sentiment = 'H'\n",
    "    elif sentiment == 'low':\n",
    "        sentiment = 'L'\n",
    "    else:\n",
    "        sentiment = 'M'\n",
    "    user_sentiment[name] = sentiment\n",
    "    \n",
    "risk_mainstream = {}\n",
    "for name, risk in data_risk_mainstream[['user_name', 'risk']].values:\n",
    "    if risk == 'yes':\n",
    "        risk = 'R'\n",
    "    else:\n",
    "        risk = 'NR'\n",
    "    risk_mainstream[name] = risk\n",
    "    \n",
    "risk_wemedia = {}\n",
    "for name, risk in data_risk_wemedia[['user_name', 'risk']].values:\n",
    "    # 对risk进行重编码\n",
    "    if risk == 'yes':\n",
    "        risk = 'R'\n",
    "    else:\n",
    "        risk = 'NR'\n",
    "    risk_wemedia[name] = risk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the repost number distribution\n",
    "\n",
    "# Ensure that Times New Roman is used as the font for all plot text\n",
    "plt.rcParams['font.family'] = 'Times New Roman'\n",
    "plt.rcParams['font.size'] = 12  # You can adjust the base font size here if needed\n",
    "\n",
    "# Assuming 'forwarding' DataFrame and 'date' column are already defined\n",
    "# Create a 'year_month' column for grouping by year and month\n",
    "forwarding['year_month'] = forwarding['date'].dt.to_period('M')\n",
    "\n",
    "# Calculate the count of forwarding actions per month\n",
    "monthly_count = forwarding.groupby('year_month').size()\n",
    "\n",
    "# Start plotting\n",
    "plt.figure(figsize=(12, 8))  # Set the figure size for better detail\n",
    "\n",
    "# Plot data\n",
    "plt.plot(monthly_count.index.to_timestamp(), monthly_count.values, color='royalblue', marker='o', linestyle='-')\n",
    "\n",
    "# Improve the formatting of the x-axis to handle dates\n",
    "plt.gca().xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m'))\n",
    "plt.gca().xaxis.set_major_locator(mdates.MonthLocator(interval=3))  # Adjust interval for less crowded x-axis\n",
    "plt.xticks(rotation=45, ha='right')  # Rotate the x-axis labels for better readability\n",
    "\n",
    "# Adding a title and labels with increased font sizes for readability\n",
    "plt.title('Monthly Forwarding Count Distribution', fontsize=16, fontweight='bold')\n",
    "plt.xlabel('Date', fontsize=14)\n",
    "plt.ylabel('Forwarding Count', fontsize=14)\n",
    "\n",
    "# Optionally, set the x-axis to only have a maximum number of date ticks\n",
    "plt.gca().xaxis.set_major_locator(MaxNLocator(6))\n",
    "\n",
    "# Remove grid lines for a cleaner look\n",
    "plt.grid(False)\n",
    "\n",
    "# Enhancing visual style by removing top and right borders\n",
    "sns.despine()\n",
    "\n",
    "# Save the figure with a high resolution of 300 DPI\n",
    "plt.savefig('./graph/forwarding_count_distribution.png', format='png', dpi=300, bbox_inches='tight')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    \"\"\"\n",
    "    A node in a network that represents an entity, which could be a media outlet or an individual,\n",
    "    capable of influencing or being influenced in terms of risk and sentiment.\n",
    "    \"\"\"\n",
    "    def __init__(self, name, node_type, risk=None, sentiment=None, sigma=None):\n",
    "        \"\"\"\n",
    "        Initializes a new instance of the Node class.\n",
    "        \n",
    "        Args:\n",
    "            name (str): The unique name or identifier for the node.\n",
    "            node_type (str): The type of the node (e.g., 'm_media', 'w_media', 'o_people').\n",
    "            risk (str, optional): Initial risk status of the node (e.g., 'R', 'NR'). Defaults to None.\n",
    "            sentiment (str, optional): Initial sentiment of the node (e.g., 'H', 'M', 'L'). Defaults to None.\n",
    "            sigma (float, optional): A threshold parameter used in decision-making processes. Defaults to None.\n",
    "        \"\"\"\n",
    "        self.name = name\n",
    "        self.type = node_type\n",
    "        self.risk = risk\n",
    "        self.sentiment = sentiment\n",
    "        self.sigma = sigma\n",
    "        self.influencers = []  # List to store nodes that influence this node.\n",
    "\n",
    "    def add_influencer(self, influencer):\n",
    "        \"\"\"\n",
    "        Adds a node to the list of influencers for this node.\n",
    "        \n",
    "        Args:\n",
    "            influencer (Node): The node to be added as an influencer.\n",
    "        \"\"\"\n",
    "        self.influencers.append(influencer)\n",
    "\n",
    "    def calculate_n_major(self):\n",
    "        \"\"\"\n",
    "        Calculates the highest number of similar sentiments among the influencers of this node.\n",
    "        \n",
    "        Returns:\n",
    "            int: The count of the most frequent sentiment category among the influencers.\n",
    "        \"\"\"\n",
    "        sentiment_counter = Counter(inf.sentiment for inf in self.influencers if inf.sentiment)\n",
    "        return max(sentiment_counter.values()) if sentiment_counter else 0\n",
    "\n",
    "    def calculate_influencers_sentiment_distribution(self):\n",
    "        \"\"\"\n",
    "        Calculates the distribution of sentiments among the influencers.\n",
    "        \n",
    "        Returns:\n",
    "            tuple: A tuple containing counts of high, low, and middle sentiments respectively.\n",
    "        \"\"\"\n",
    "        sentiment_counts = Counter(inf.sentiment for inf in self.influencers if inf.sentiment)\n",
    "        return sentiment_counts.get('H', 0), sentiment_counts.get('L', 0), sentiment_counts.get('M', 0)\n",
    "\n",
    "    def update_m_media(self, params):\n",
    "        \"\"\"\n",
    "        Updates the risk status of this node if it is of type 'm_media' based on external parameters.\n",
    "        \n",
    "        Args:\n",
    "            params (dict): Parameters that include alpha and the differential sentiment count.\n",
    "        \"\"\"\n",
    "        if self.type == 'm_media':\n",
    "            alpha = params['alpha']\n",
    "            d = params['d']  # Differential in sentiment counts\n",
    "            if d > 0:\n",
    "                Pu = 1 - math.exp(-alpha * d)\n",
    "                if Pu > np.random.random():\n",
    "                    self.risk = 'NR'\n",
    "\n",
    "    def update_w_media(self, params):\n",
    "        \"\"\"\n",
    "        Updates the risk status for 'w_media' type nodes based on the sentiment distribution among influencers.\n",
    "        \n",
    "        Args:\n",
    "            params (dict): Parameters including the beta coefficient and normalized sentiment metrics.\n",
    "        \"\"\"\n",
    "        if self.type == 'w_media':\n",
    "            n_high_p = params['n_high'] / (params['n_high'] + params['n_low'] + params['n_middle'])\n",
    "            Pv = 1 / (1 + np.exp(-params['beta'] * n_high_p))\n",
    "            if Pv > np.random.uniform(0.5, 1):\n",
    "                self.risk = 'R'\n",
    "            else:\n",
    "                self.risk = 'NR'\n",
    "\n",
    "    def update_o_people(self, params):\n",
    "        \"\"\"\n",
    "        Updates the sentiment of 'o_people' type nodes based on the influence of risk status from influencers.\n",
    "        \n",
    "        Args:\n",
    "            params (dict): Contains parameters such as theta which influences the sentiment update process.\n",
    "        \"\"\"\n",
    "        if self.type == 'o_people':\n",
    "            r_1 = sum(1 if inf.risk == 'R' else -1 for inf in self.influencers) / (len(self.influencers) + 1)\n",
    "            g_1 = 1 / (1 + np.exp(-params['theta'] * r_1)) if r_1 != 0 else 0\n",
    "            if g_1 > np.random.uniform(0, 1):\n",
    "                self.sentiment = np.random.choice(['L', 'H'])\n",
    "            else:\n",
    "                self.sentiment = 'M'\n",
    "            \n",
    "            sentiments = [inf.sentiment for inf in self.influencers if inf.sentiment]\n",
    "            eta = sum(1 for inf in self.influencers if inf.sentiment == self.sentiment) / len(sentiments) if sentiments else 0\n",
    "            if eta > self.sigma:\n",
    "                sentiment_counter = Counter([inf.sentiment for inf in self.influencers if inf.sentiment])\n",
    "                self.sentiment = max(sentiment_counter, key=sentiment_counter.get)\n",
    "    \n",
    "    def update_w_media_gv(self, params, government_effect_w):\n",
    "        \"\"\"\n",
    "        Updates the risk status for 'w_media' type nodes considering government intervention effect alongside the basic sentiment distribution.\n",
    "        \n",
    "        Args:\n",
    "            params (dict): Parameters including sentiment proportions and beta coefficient.\n",
    "            government_effect_w (float): Adjustment to the influence probability due to government intervention.\n",
    "        \"\"\"\n",
    "        if self.type == 'w_media':\n",
    "            n_high_p = params['n_high'] / (params['n_high'] + params['n_low'] + params['n_middle'])\n",
    "            Pv = 1 / (1 + np.exp(-params['beta'] * (n_high_p + params['risk_m']))) - government_effect_w\n",
    "            if Pv > np.random.uniform(0.5, 1):\n",
    "                self.risk = 'R'\n",
    "            else:\n",
    "                self.risk = 'NR'\n",
    "\n",
    "    def update_o_people_gv(self, params, government_effect_o):\n",
    "        \"\"\"\n",
    "        Updates the sentiment of 'o_people' type nodes by considering both the sentiment influence of their influencers and government effects.\n",
    "        \n",
    "        Args:\n",
    "            params (dict): Includes parameters for the sentiment update calculation.\n",
    "            government_effect_o (float): Government intervention effect that influences sentiment adjustment.\n",
    "        \"\"\"\n",
    "        if self.type == 'o_people':\n",
    "            r_1 = sum(1 if inf.risk == 'R' else -1 for inf in self.influencers) / (len(self.influencers) + 1)\n",
    "            g_1 = 1 / (1 + np.exp(-params['theta'] * r_1)) if r_1 != 0 else 0\n",
    "            g_1 = g_1 - government_effect_o  # Apply government effect\n",
    "            if g_1 > np.random.uniform(0, 1):\n",
    "                self.sentiment = np.random.choice(['L', 'H'])\n",
    "            else:\n",
    "                self.sentiment = 'M'\n",
    "            \n",
    "            sentiments = [inf.sentiment for inf in self.influencers if inf.sentiment]\n",
    "            eta = sum(1 for inf in self.influencers if inf.sentiment == self.sentiment) / len(sentiments) if sentiments else 0\n",
    "            if eta > self.sigma:\n",
    "                sentiment_counter = Counter([inf.sentiment for inf in self.influencers if inf.sentiment])\n",
    "                self.sentiment = max(sentiment_counter, key=sentiment_counter.get)\n",
    "\n",
    "    def forward(self, params):\n",
    "        \"\"\"\n",
    "        Processes one step of updates for this node, calling appropriate methods based on node type.\n",
    "        \n",
    "        Args:\n",
    "            params (dict): Parameters required for updating node statuses.\n",
    "        \"\"\"\n",
    "        self.update_m_media(params)\n",
    "        self.update_w_media(params)\n",
    "        self.update_o_people(params)\n",
    "\n",
    "    def forward_gv(self, params, government_effect_w, government_effect_o):\n",
    "        \"\"\"\n",
    "        Processes one step of updates for this node under government intervention, calling updates with government effects.\n",
    "        \n",
    "        Args:\n",
    "            params (dict): Parameters required for updating node statuses.\n",
    "            government_effect_w (float): Government effect applied to 'w_media' nodes.\n",
    "            government_effect_o (float): Government effect applied to 'o_people' nodes.\n",
    "        \"\"\"\n",
    "        self.update_m_media(params)\n",
    "        self.update_w_media_gv(params, government_effect_w)\n",
    "        self.update_o_people_gv(params, government_effect_o)\n",
    "\n",
    "\n",
    "\n",
    "class Network:\n",
    "    def __init__(self):\n",
    "        \"\"\"Initialize the network with an empty dictionary to store nodes.\"\"\"\n",
    "        self.nodes = {}\n",
    "\n",
    "    def add_node(self, node):\n",
    "        \"\"\"Add a node to the network using the node's name as the key.\"\"\"\n",
    "        self.nodes[node.name] = node\n",
    "\n",
    "    def calculate_sentiment_distribution(self):\n",
    "        \"\"\"Calculate the distribution of sentiments among 'o_people' type nodes.\"\"\"\n",
    "        sentiment_counts = Counter(node.sentiment for node in self.nodes.values() if node.type == 'o_people')\n",
    "        return sentiment_counts\n",
    "\n",
    "    def simulate_step(self, alpha, beta, theta):\n",
    "        \"\"\"Simulate a single step of the network dynamics with given parameters alpha, beta, and theta.\"\"\"\n",
    "        sentiment_counts = self.calculate_sentiment_distribution()\n",
    "        property = self.calculate_state_proportions()\n",
    "        risk_m = property['m_media']['R']\n",
    "        d = (sentiment_counts['H'] + sentiment_counts['L'] - sentiment_counts['M']) / (sentiment_counts['M'] + 1)\n",
    "        params = {\n",
    "            'alpha': alpha, 'beta': beta, 'theta': theta, 'd': d,\n",
    "            'n_high': sentiment_counts['H'], 'n_low': sentiment_counts['L'], \n",
    "            'n_middle': sentiment_counts['M'], 'risk_m': risk_m\n",
    "        }\n",
    "        for node in self.nodes.values():\n",
    "            node.forward(params)\n",
    "    \n",
    "    def simulate_step_gv(self, alpha, beta, theta, government_effect_w, government_effect_o):\n",
    "        \"\"\"Simulate a single step considering government effects on 'w_media' and 'o_people'.\"\"\"\n",
    "        sentiment_counts = self.calculate_sentiment_distribution()\n",
    "        property = self.calculate_state_proportions()\n",
    "        risk_m = property['m_media']['R']\n",
    "        d = (sentiment_counts['H'] + sentiment_counts['L'] - sentiment_counts['M']) / (sentiment_counts['M'] + 1)\n",
    "        params = {\n",
    "            'alpha': alpha, 'beta': beta, 'theta': theta, 'd': d,\n",
    "            'n_high': sentiment_counts['H'], 'n_low': sentiment_counts['L'], \n",
    "            'n_middle': sentiment_counts['M'], 'risk_m': risk_m\n",
    "        }\n",
    "        for node in self.nodes.values():\n",
    "            node.forward_gv(params, government_effect_w, government_effect_o)\n",
    "\n",
    "    def simulate_steps(self, steps, alpha, beta, theta, delta_1, delta_2, delta_3, delta_4, gamma_1, gamma_2, gamma_3, gamma_4, cutoff_1=31, cutoff_2=51, duration_1=15, duration_2=15, duration_3=30):\n",
    "        \"\"\"Simulate multiple steps of the network dynamics over a specified number of steps and time intervals with varying government effects.\"\"\"\n",
    "        # Calculate initial distribution and setup initial history tracking\n",
    "        middle_1 = round(cutoff_1 + 0.5 * duration_1)\n",
    "        middle_2 = round(cutoff_2 + 0.5 * duration_2)\n",
    "        middle_3 = round(cutoff_2 + 0.5 * (duration_3 + duration_2))\n",
    "        ini_distribution = self.calculate_state_proportions()\n",
    "        history = [ini_distribution]\n",
    "\n",
    "        for t in range(steps):\n",
    "            # Check for first special effect period\n",
    "            if cutoff_1 <= t < cutoff_1 + duration_1 + 1:\n",
    "                government_effect_w = delta_1 * ((t - middle_1) / duration_1) ** 2 + delta_2\n",
    "                government_effect_o = gamma_1 * ((t - middle_1) / duration_1) ** 2 + gamma_2\n",
    "                self.simulate_step_gv(alpha, beta, theta, government_effect_w, government_effect_o)\n",
    "                proportions = self.calculate_state_proportions()\n",
    "                history.append(proportions)\n",
    "\n",
    "            # Check for second special effect period\n",
    "            elif cutoff_2 <= t < cutoff_2 + duration_2 + duration_3 + 1:\n",
    "                if t < cutoff_2 + duration_2 + 1:\n",
    "                    government_effect_w = delta_3 * ((t - middle_2) / duration_2) ** 2 + delta_4\n",
    "                    government_effect_o = gamma_3 * ((t - middle_3) / (duration_2 + duration_3)) ** 2 + gamma_4\n",
    "                    self.simulate_step_gv(alpha, beta, theta, government_effect_w, government_effect_o)\n",
    "                else:\n",
    "                    government_effect_o = gamma_3 * ((t - middle_3) / (duration_2 + duration_3)) ** 2 + gamma_4\n",
    "                    self.simulate_step_gv(alpha, beta, theta, 0, government_effect_o)\n",
    "\n",
    "                proportions = self.calculate_state_proportions()\n",
    "                history.append(proportions)\n",
    "\n",
    "            else:\n",
    "                # Regular simulation step\n",
    "                self.simulate_step(alpha, beta, theta)\n",
    "                proportions = self.calculate_state_proportions()\n",
    "                history.append(proportions)\n",
    "\n",
    "        return history\n",
    "\n",
    "    def calculate_state_proportions(self):\n",
    "        \"\"\"Calculate the proportion of each state within each node type ('m_media', 'w_media', 'o_people').\"\"\"\n",
    "        states = {\n",
    "            'm_media': Counter({'R': 0, 'NR': 0}),\n",
    "            'w_media': Counter({'R': 0, 'NR': 0}),\n",
    "            'o_people': Counter({'H': 0, 'M': 0, 'L': 0})\n",
    "        }\n",
    "\n",
    "        for node in self.nodes.values():\n",
    "            if node.type in ['m_media', 'w_media']:\n",
    "                states[node.type][node.risk] += 1\n",
    "            elif node.type == 'o_people':\n",
    "                states[node.type][node.sentiment] += 1\n",
    "\n",
    "        proportions = {type: {state: count / sum(states[type].values()) for state, count in states[type].items()} for type in states}\n",
    "        return proportions\n",
    "\n",
    "    def display(self):\n",
    "        \"\"\"Display the current status of each node in the network.\"\"\"\n",
    "        for name, node in self.nodes.items():\n",
    "            print(f\"{name} ({node.type}): Risk={node.risk}, Sentiment={node.sentiment}, Influencers={[inf.name for inf in node.influencers]}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "\n",
    "np.random.seed(1234)  # Set the random seed for reproducibility\n",
    "network = Network()  # Create an instance of the Network class\n",
    "\n",
    "# Probabilities for different node types\n",
    "p_w_media = [345/348, 3/348]  # Probability distribution for w_media nodes\n",
    "p_m_media = [1, 0]  # Probability distribution for m_media nodes (always 'R')\n",
    "p_o_people = [169/537, 210/537, 158/537]  # Probability distribution for o_people nodes\n",
    "\n",
    "# Create and add nodes from links data for origins\n",
    "for origin_name, group in links.groupby(['origin_name']):\n",
    "    node_type = group['o_type'].values[0]\n",
    "    origin_name_ = group['origin_name'].values[0]\n",
    "\n",
    "    if node_type == 'w_media':\n",
    "        risk = risk_wemedia.get(origin_name_, None)\n",
    "        if risk is None:  # Assign random risk if not present\n",
    "            risk = np.random.choice(['R', 'NR'], p=p_w_media)\n",
    "        node = Node(name=origin_name_, node_type=node_type, risk=risk)\n",
    "        \n",
    "    elif node_type == 'm_media':\n",
    "        risk = risk_mainstream.get(origin_name_, None)\n",
    "        if risk is None:\n",
    "            risk = np.random.choice(['R', 'NR'], p=p_m_media)\n",
    "        node = Node(name=origin_name_, node_type=node_type, risk='R')\n",
    "        \n",
    "    elif node_type == 'o_people':\n",
    "        sentiment = np.random.choice(['H', 'M', 'L'], p=p_o_people)\n",
    "        node = Node(name=origin_name_, node_type=node_type, sentiment=sentiment, sigma=np.random.random())\n",
    "\n",
    "    network.add_node(node)\n",
    "\n",
    "# Create and add nodes from links data for users\n",
    "for user_name, group in links.groupby(['user_name']):\n",
    "    node_type = group['d_type'].values[0]\n",
    "    user_name_ = group['user_name'].values[0]\n",
    "\n",
    "    if node_type == 'o_people':\n",
    "        sentiment = np.random.choice(['H', 'M', 'L'], p=p_o_people)\n",
    "        node = Node(name=user_name_, node_type=node_type, sentiment=sentiment, sigma=np.random.random())\n",
    "        \n",
    "    elif node_type == 'm_media':\n",
    "        risk = risk_mainstream.get(user_name_, None)\n",
    "        if risk is None:\n",
    "            risk = np.random.choice(['R', 'NR'], p=p_m_media)\n",
    "        node = Node(name=user_name_, node_type=node_type, risk=risk)\n",
    "        \n",
    "    else:  # Assume w_media or another unspecified category\n",
    "        risk = risk_wemedia.get(user_name_, None)\n",
    "        if risk is None:\n",
    "            risk = np.random.choice(['R', 'NR'], p=p_w_media)\n",
    "        node = Node(name=user_name_, node_type=node_type, risk=risk)\n",
    "        \n",
    "    network.add_node(node)\n",
    "\n",
    "# Set up bidirectional influencer relationships\n",
    "for origin_name, user_name in links[['origin_name', 'user_name']].values:\n",
    "    object_1 = network.nodes[origin_name]\n",
    "    object_2 = network.nodes[user_name]\n",
    "    object_1.add_influencer(object_2)\n",
    "    object_2.add_influencer(object_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joblib.dump(network, 'network_sigmoid_wt_v4.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# best_param = results_df[results_df['error'] == results_df['error'].min()]['param']\n",
    "# network_ini = joblib.load('network.pkl')\n",
    "# history = network_ini.simulate_steps(95, *best_param.values[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 保存nodes为json文件\n",
    "# with open('nodes.json', 'w') as f:\n",
    "#     json.dump(nodes, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Calibration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Setup and Model Definition\n",
    "\n",
    "The model setup and definitions, including classes for nodes and networks, have been provided. We first need to ensure that all necessary libraries are imported at the beginning of the script. I noticed that `numpy` is used as `np` in the code, but it was not explicitly imported, so we have added this import.\n",
    "\n",
    "### Step 2: Parameter Sampling\n",
    "\n",
    "Define a function to sample parameter combinations uniformly. Sampling should be done in the promising regions of the parameter space for `alpha`, `beta`, and `theta`. Since the specific promising regions are not provided, I have used example ranges for demonstration, but these should be adjusted according to your prior analysis or the specific content of Supplementary Material Figure 13.\n",
    "\n",
    "### Step 3: Simulation Execution\n",
    "\n",
    "For each parameter combination, we will use a set of predetermined random seeds to run the simulation and compare the simulation results with the empirical dataset (`m_media_risk_rate`, `w_media_risk_rate`, `o_people_middle_rate`).\n",
    "\n",
    "### Step 4: Selection of Parameter Combinations\n",
    "\n",
    "After running the simulations, we will apply a specified error threshold to select the acceptable parameter combinations.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def sample_parameter_combinations_lhs(num_samples=10000):\n",
    "    \"\"\"\n",
    "    Generate parameter combinations using Latin Hypercube Sampling (LHS).\n",
    "    \n",
    "    Parameters:\n",
    "        num_samples (int): The number of parameter samples to generate.\n",
    "        \n",
    "    Returns:\n",
    "        list: A list of tuples, each containing a sampled parameter combination.\n",
    "    \"\"\"\n",
    "    sampler = LatinHypercube(d=11)  # Dimensionality of parameter space\n",
    "    samples = sampler.random(n=num_samples)\n",
    "\n",
    "    # Mapping samples from [0, 1] interval to the actual parameter ranges\n",
    "    alpha_range = (2, 5)\n",
    "    beta_range = (10, 20)\n",
    "    theta_range = (0.01, 0.4)\n",
    "    delta_ranges = [(-1, -0.01), (0.1, 0.2)] * 2\n",
    "    gamma_ranges = [(-1, -0.01), (0.1, 0.2)] * 2\n",
    "\n",
    "    # Unpack all parameters and map them to their respective ranges\n",
    "    parameters = [alpha_range, beta_range, theta_range] + delta_ranges + gamma_ranges\n",
    "    return [\n",
    "        tuple(sample[i] * (param_range[1] - param_range[0]) + param_range[0] for i, param_range in enumerate(parameters))\n",
    "        for sample in samples\n",
    "    ]\n",
    "\n",
    "def simulate_network_for_parameters(network, param, num_seeds=3):\n",
    "    \"\"\"\n",
    "    Run network simulation for given parameters across multiple random seeds and summarize the results.\n",
    "    \n",
    "    Parameters:\n",
    "        network: The network instance to simulate.\n",
    "        param (tuple): The parameters for the simulation.\n",
    "        num_seeds (int): Number of different seeds to simulate.\n",
    "        \n",
    "    Returns:\n",
    "        dict: A dictionary containing the fit status, parameters used, and the average error.\n",
    "    \"\"\"\n",
    "    errors = []\n",
    "    for seed in range(num_seeds):\n",
    "        np.random.seed(seed)\n",
    "        # Reset network to initial state if necessary\n",
    "        simulation_results = network.simulate_steps(95, *param)  # Assuming simulation for 96 days\n",
    "\n",
    "        # Process simulation results weekly\n",
    "        weekly_results = {i: {} for i in range(14)}  # Preparing dictionary to store weekly data\n",
    "        for i in range(14):\n",
    "            week_slice = simulation_results[i*7:(i+1)*7]\n",
    "            weekly_results[i] = {\n",
    "                'm_media_risk': np.mean([day['m_media']['R'] for day in week_slice]),\n",
    "                'w_media_risk': np.mean([day['w_media']['R'] for day in week_slice]),\n",
    "                'o_people_high': np.mean([day['o_people']['H'] for day in week_slice]),\n",
    "                'o_people_middle': np.mean([day['o_people']['M'] for day in week_slice])\n",
    "            }\n",
    "\n",
    "        # Calculate error comparing to empirical data\n",
    "        error = calculate_error(weekly_results, empirical_data)\n",
    "        errors.append(error)\n",
    "\n",
    "    avg_error = np.mean(errors)\n",
    "    return {\n",
    "        'result': 'fitting' if meets_error_criteria(avg_error) else 'not fitting',\n",
    "        'param': param,\n",
    "        'error': avg_error\n",
    "    }\n",
    "\n",
    "def calculate_error(simulation_results, empirical_data):\n",
    "    \"\"\"\n",
    "    Calculate error between simulation results and empirical data.\n",
    "    \n",
    "    Parameters:\n",
    "        simulation_results (dict): The simulated data.\n",
    "        empirical_data (dict): The observed empirical data.\n",
    "        \n",
    "    Returns:\n",
    "        float: The average absolute error.\n",
    "    \"\"\"\n",
    "    error_sum = sum(\n",
    "        abs(simulation_results[key][step] - empirical_data[key][step])\n",
    "        for key in simulation_results\n",
    "        for step in simulation_results[key]\n",
    "    )\n",
    "    return error_sum / 48  # Total number of time steps considered\n",
    "\n",
    "def meets_error_criteria(error):\n",
    "    \"\"\"\n",
    "    Check if the given error meets the acceptable threshold.\n",
    "    \n",
    "    Parameters:\n",
    "        error (float): The calculated error.\n",
    "        \n",
    "    Returns:\n",
    "        bool: True if error is within acceptable limits, else False.\n",
    "    \"\"\"\n",
    "    error_threshold = 0.07  # Example threshold, adjust based on requirements\n",
    "    return error < error_threshold\n",
    "\n",
    "def plot_history_with_empirical(history, empirical_data):\n",
    "    \"\"\"\n",
    "    Plot the simulation history alongside empirical data points.\n",
    "    \n",
    "    Parameters:\n",
    "        history (list): List of dictionaries containing the simulated state proportions over time.\n",
    "        empirical_data (dict): Dictionary containing empirical data points.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    labels = {'m_media_R': 'Mainstream Media Risk', 'w_media_R': 'Web Media Risk', 'o_people_H': 'High Sentiment', 'o_people_M': 'Middle Sentiment', 'l_people_L': 'Low Sentiment'}\n",
    "    time_steps = range(len(history))\n",
    "\n",
    "    # Plotting simulated data\n",
    "    for label, data_key in labels.items():\n",
    "        plt.plot(time_steps, [day.get(data_key, 0) for day in history], label=label)\n",
    "\n",
    "    # Plot empirical data points\n",
    "    for week, data_point in empirical_data.items():\n",
    "        day = week * 7\n",
    "        plt.scatter([day] * len(data_point), list(data_point.values()), label=f'Week {week} Empirical Data')\n",
    "\n",
    "    plt.legend()\n",
    "    plt.title('Comparison of Simulated and Empirical Data')\n",
    "    plt.xlabel('Time (days)')\n",
    "    plt.ylabel('Proportion')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load empirical data from CSV files\n",
    "w_media_empirical = pd.read_csv('./output/empirical/w_media_risk_data_7days.csv')\n",
    "o_people_empirical = pd.read_csv('./output/empirical/sentiment_data_7days.csv')\n",
    "m_media_empirical = pd.read_csv('./output/empirical/m_media_risk_data_7days.csv')\n",
    "\n",
    "# Convert data into dictionaries mapping period identifiers to risk or sentiment proportions\n",
    "emp_w_risk_p = dict(zip(w_media_empirical['period_id_3d'], w_media_empirical['risk_p']))\n",
    "emp_m_risk_p = dict(zip(m_media_empirical['period_id_3d'], m_media_empirical['risk_p']))\n",
    "emp_sentiment_high_p = dict(zip(o_people_empirical['period_id_3d'], o_people_empirical['high_p']))\n",
    "emp_sentiment_middle_p = dict(zip(o_people_empirical['period_id_3d'], o_people_empirical['middle_p']))\n",
    "emp_sentiment_low_p = dict(zip(o_people_empirical['period_id_3d'], o_people_empirical['low_p']))\n",
    "\n",
    "# Aggregate the empirical data into a single dictionary for ease of use in simulations and analysis\n",
    "empirical_data = {\n",
    "    'w_risk_p': emp_w_risk_p,\n",
    "    'm_risk_p': emp_m_risk_p,\n",
    "    'sentiment_high_p': emp_sentiment_high_p,\n",
    "    'sentiment_middle_p': emp_sentiment_middle_p,\n",
    "    # Uncomment the next line if low sentiment proportions are needed for further analysis\n",
    "    # 'sentiment_low_p': emp_sentiment_low_p\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(2345)\n",
    "all_params = sample_parameter_combinations_lhs(5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-initialized network from a saved file outside of the loop to avoid repetitive loading if applicable\n",
    "network = joblib.load('network_sigmoid_wt_v4.pkl')\n",
    "\n",
    "# Initialize an empty list to store simulation results\n",
    "results = []\n",
    "\n",
    "# Loop through all parameters starting from the 5508th parameter using tqdm for progress tracking\n",
    "for param in tqdm.tqdm(all_params[5507:], desc='Simulating'):\n",
    "    try:\n",
    "        # Perform network simulation with the current parameter set\n",
    "        result = simulate_network_for_parameters(network, param, num_seeds=3)\n",
    "        \n",
    "        # Store the result of the simulation\n",
    "        results.append(result)\n",
    "        \n",
    "        # Print the parameters and results if the simulation was successful\n",
    "        if result['result'] != 'not fitting':\n",
    "            print(f\"Successful Parameter Set: {param}, Result: {result}\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        # Print error message if an exception occurs\n",
    "        print(f\"Error occurred for parameter {param}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame(results).sort_values('error')\n",
    "results_df['alpha'] = results_df['param'].apply(lambda x: x[0])\n",
    "results_df['beta'] = results_df['param'].apply(lambda x: x[1])\n",
    "results_df['theta'] = results_df['param'].apply(lambda x: x[2])\n",
    "results_df['delta_1'] = results_df['param'].apply(lambda x: x[3])\n",
    "results_df['delta_2'] = results_df['param'].apply(lambda x: x[4])\n",
    "results_df['delta_3'] = results_df['param'].apply(lambda x: x[5])\n",
    "results_df['delta_4'] = results_df['param'].apply(lambda x: x[6])\n",
    "results_df['gamma_1'] = results_df['param'].apply(lambda x: x[7])\n",
    "results_df['gamma_2'] = results_df['param'].apply(lambda x: x[8])\n",
    "results_df['gamma_3'] = results_df['param'].apply(lambda x: x[9])\n",
    "results_df['gamma_4'] = results_df['param'].apply(lambda x: x[10])\n",
    "results_df['error'].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df.to_csv('./results_df.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.read_csv('./results_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df[results_df['error'] == results_df['error'].min()]['param'].values[0]\n",
    "\n",
    "# 将字符串转换为元组\n",
    "import ast\n",
    "best_param = ast.literal_eval(results_df[results_df['error'] == results_df['error'].min()]['param'].values[0])\n",
    "alpha, beta, theta, delta_1, delta_2, delta_3, delta_4, gamma_1, gamma_2, gamma_3, gamma_4 = best_param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "empirical_data_another = {\n",
    "    'w_risk_p': emp_w_risk_p,\n",
    "    'm_risk_p': emp_m_risk_p,\n",
    "    'sentiment_high_p': emp_sentiment_high_p,\n",
    "    'sentiment_middle_p': emp_sentiment_middle_p,\n",
    "    'sentiment_low_p': emp_sentiment_low_p\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# alpha, beta, theta = (3.17716490514395, 14.9066214771117403, 10.10791529106301318)\n",
    "# param = alpha, beta, theta\n",
    "# print(np.random.uniform(0.1, 0.15))\n",
    "network_ini = joblib.load('network_sigmoid_wt_v4.pkl')\n",
    "\n",
    "history = network_ini.simulate_steps(91, alpha, beta, theta, delta_1, delta_2, delta_3, delta_4, gamma_1, gamma_2, gamma_3, gamma_4)\n",
    "\n",
    "plot_history_with_empirical(history, empirical_data_another)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history\n",
    "\n",
    "# 我要计算每个情绪状态的误差情况\n",
    "error_for_sentiment_high = 0\n",
    "error_for_sentiment_middle = 0\n",
    "error_for_sentiment_low = 0\n",
    "\n",
    "for i in range(14):\n",
    "    error_for_sentiment_high += abs(emp_sentiment_high_p[i] - history[i]['o_people']['H'])\n",
    "    error_for_sentiment_middle += abs(emp_sentiment_middle_p[i] - history[i]['o_people']['M'])\n",
    "    error_for_sentiment_low += abs(emp_sentiment_low_p[i] - history[i]['o_people']['L'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 计算每个媒体风险的误差情况\n",
    "error_for_w_risk = 0\n",
    "error_for_m_risk = 0\n",
    "\n",
    "#注意有些时间步没有数据，所以要判断一下\n",
    "for i in range(14):\n",
    "    if i in emp_w_risk_p:\n",
    "        error_for_w_risk += abs(emp_w_risk_p[i] - history[i]['w_media']['R'])\n",
    "    if i in emp_m_risk_p:\n",
    "        error_for_m_risk += abs(emp_m_risk_p[i] - history[i]['m_media']['R'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 计算各自的平均误差，注意有些长度不是14，并打印\n",
    "print(f'error_for_sentiment_high: {error_for_sentiment_high/14}')\n",
    "print(f'error_for_sentiment_middle: {error_for_sentiment_middle/14}')\n",
    "print(f'error_for_sentiment_low: {error_for_sentiment_low/14}')\n",
    "print(f'error_for_w_risk: {error_for_w_risk/14}')\n",
    "print(f'error_for_m_risk: {error_for_m_risk/6}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "empirical_data_another = {\n",
    "    'w_risk_p': emp_w_risk_p,\n",
    "    'm_risk_p': emp_m_risk_p,\n",
    "    'sentiment_high_p': emp_sentiment_high_p,\n",
    "    'sentiment_middle_p': emp_sentiment_middle_p,\n",
    "    'sentiment_low_p': emp_sentiment_low_p\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 找到最小error的参数\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df.to_csv('results_df.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ordinary",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
